{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deploy BLOOM to Amazon SageMaker using Text-Generation-Inference\n",
    "\n",
    "This is an example on how to deploy the open-source LLMs, like [BLOOM](bigscience/bloom) to Amazon SageMaker for inference. We will deploy BLOOM 176B to Amazon SageMake for real-time inference using Hugging Face new LLM solution [text-generation-inference](https://github.com/huggingface/text-generation-inference). \n",
    "\n",
    "The example covers:\n",
    "1. Setup development environment\n",
    "2. Create `HuggingFace` model with TGI container\n",
    "3. Deploy BLOOM to Amazon SageMaker\n",
    "4. Run inference on BLOOM with different parameters\n",
    "5. Run token streaming on BLOOM\n",
    "6. Clean up\n",
    "\n",
    "## What is Text Generation Inference?\n",
    "\n",
    "[Text Generation Inference](https://github.com/huggingface/text-generation-inference) is a library built by Hugging Face to offer an end-to-end optimized solution to run inference on open source LLMs, already powering Hugging Face services running at scale such as the Hugging Face Inference API for BLOOM, GPT-NeoX, SantaCoder, and many more LLMs. In addition, Text Generation Inference is already used by customers such as IBM, Grammarly, and the Open-Assistant initiative. \\\n",
    "Text Generation Inference implements optimization for all supported model architectures, including:\n",
    "* Tensor Parallelism and custom cuda kernels\n",
    "* Quantization\n",
    "* Dynamic batching of incoming requests for increased total throughput \n",
    "* Accelerated weight loading (start-up time) with safetensors\n",
    "* Logits warpers (temperature scaling, topk, repetition penalty ...)\n",
    "* Stop sequences, Log probabilities\n",
    "* Token streaming using Server-Sent Events (SSE)\n",
    "\n",
    "Officially supported model architectures are currently: \n",
    "* [BLOOM](https://huggingface.co/bigscience/bloom) / [BLOOMZ](https://huggingface.co/bigscience/bloomz)\n",
    "* [MT0-XXL](https://huggingface.co/bigscience/mt0-xxl)\n",
    "* [Galactica](https://huggingface.co/facebook/galactica-120b)\n",
    "* [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "* [GPT-Neox 20B](https://huggingface.co/EleutherAI/gpt-neox-20b) (joi, pythia, lotus, rosey, chip)\n",
    "* [FLAN-T5-XXL](https://huggingface.co/google/flan-t5-xxl) (T5-11B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy BLOOM to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create `HuggingFace` model with TGI container\n",
    "\n",
    "As of today the text-generation-inference container is not yet available natively inside `sagemaker`. We will use the `HuggingFaceModel` model class with a custom `image_uri` pointing to the registry image of the text-generation-inference container. The text-generation-inference container is available in Github Repository as package. You can find more information about the container [here](https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference). \n",
    "\n",
    "To make the use with SageMaker easier we uploaded a version of the container to a public ECR repository. If you want to migrate the container yourself We created a `create_ecr_contaienr.sh` script we can use to migrate the container to ECR.\n",
    "_Note: make sure you have permissions to create ECR repositories and docker running._\n",
    "```python\n",
    "image_uri = \"ghcr.io/huggingface/text-generation-inference:sagemaker-sha-631c4c8\"\n",
    "account_id = sess.account_id()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "!chmod +x create_ecr_container.sh\n",
    "!./create_ecr_container.sh {image_uri} {account_id} {region}\n",
    "\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-text-generation-inference:latest\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-generation-inference container image uri\n",
    "image_uri=\"558105141721.dkr.ecr.us-east-1.amazonaws.com/sagemaker-text-generation-inference:latest\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy BLOOM to Amazon SageMaker we need to create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. . We will use a `p4d.24xlarge` instance type with quantization enabled to deploy BLOOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "hf_model_id = \"bigscience/bloom\" # model id from huggingface.co/models\n",
    "use_quantized_model = True # wether to use quantization or not\n",
    "instance_type = \"ml.p4d.24xlarge\" # instance type to use for deployment\n",
    "number_of_gpu = 8 # number of gpus to use for inference and tensor parallelism\n",
    "health_check_timeout = 900 # Increase the timeout for the health check to 15 minutes for downloading bloom\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "bloom_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=image_uri,\n",
    "  env={\n",
    "    'HF_MODEL_ID': hf_model_id,\n",
    "    'HF_MODEL_QUANTIZE': json.dumps(use_quantized_model),\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu)\n",
    "  }\n",
    ")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.p4d.24xlarge` instance type. This instance type is required to run BLOOM 176B using int8 quantization. You can find more information about the instance types [here](https://aws.amazon.com/sagemaker/pricing/instance-types/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "predictor = bloom_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run inference on BLOOM with different parameters\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor`to run inference on our endpoint. We will run inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. As of today the text-generation-inference container supports the following parameters:\n",
    "* `temperature`: Controls randomness in the model. Lower values will make the model more deterministic and higher values will make the model more random. Default value is 1.0.\n",
    "* `max_new_tokens`: The maximum number of tokens to generate. Default value is 20, max value is 512.\n",
    "* `repetition_penalty`: Controls the likelihood of repetition.\n",
    "* `seed`: The seed to use for random generation.\n",
    "* `stop`: A list of tokens to stop the generation. The generation will stop when one of the tokens is generated.\n",
    "* `top_k`: The number of highest probability vocabulary tokens to keep for top-k-filtering. Default value is 0, which disables top-k-filtering.\n",
    "* `top_p`: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling.\n",
    "* `do_sample`: Whether or not to use sampling ; use greedy decoding otherwise. Default value is False.\n",
    "\n",
    "You can find the open api specification of the text-generation-inference container [here](https://huggingface.github.io/text-generation-inference/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your problem? What is the error message you are getting? What is the exact code you are using?'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t\"inputs\": \"Can you please let us know more details about your\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do a hello world in different languages:\n",
      "Python: print(\"hello world\")\n",
      "R: print(\"Hello world!\")\n",
      "Lisp: (format nil \"Hello world!\")\n",
      "Scheme:\n"
     ]
    }
   ],
   "source": [
    "# define payload\n",
    "prompt=\"\"\"Do a hello world in different languages:\n",
    "Python: print(\"hello world\")\n",
    "R:\"\"\"\n",
    "\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run token streaming on BLOOM\n",
    "\n",
    "Text Generation Inference supports token streaming using Server-Sent Events (SSE). This means that the text-generation-inference container will stream the generated tokens back to the client, while the generation is still running. This is useful for long generation tasks where the client wants to see the generation in real-time and gives a better user experience. \n",
    "\n",
    "To use token streaming we need to pass the `stream` parameter in our payload and use for python the `sseclient-py` library to read the stream. We cannot use the `predict` method from the `predictor` to run inference on our endpoint. We need to use the `requests` library to send the request to the endpoint using a manuall create AWS Signature Version 4. You can find more information about the AWS Signature Version 4 [here](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sseclient-py --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run inference with token streaming. We will use the `sseclient-py` library to read the stream and print the generated tokens. We wrote two helper methods, which allows us to run inference with token streaming. The first method `http_request` creates an HTTP request with a AWS Signature Version 4. The second method `stream_request` uses the `http_request` method to send the request to the endpoint and uses the `sseclient-py` library to read the stream and print the generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "import requests\n",
    "import sseclient\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "creds = credentials.get_frozen_credentials()\n",
    "\n",
    "# HTTP Request method with AWS SigV4 signing\n",
    "def http_request(url, data ,method=\"POST\", is_aws=True):\n",
    "    # set stream attribute in payload\n",
    "    data[\"stream\"] = True\n",
    "    body = json.dumps(data)\n",
    "    # define headers\n",
    "    headers = {'Content-Type': 'application/json',\"Accept\":\"text/event-stream\",'Connection': 'keep-alive'}\n",
    "    # sign request\n",
    "    if is_aws:\n",
    "        request = AWSRequest(method=method, url=url, data=body, headers=headers)\n",
    "        SigV4Auth(creds, \"sagemaker\", session.region_name).add_auth(request)\n",
    "        headers = dict(request.headers)\n",
    "    # send request\n",
    "    return requests.post(url=url, headers=headers, data=body, stream=True)\n",
    "\n",
    "# Stream request for token streamning using SSE\n",
    "def stream_request(url,data, is_aws=True, split_token=\"\"):\n",
    "    # send request\n",
    "    res = http_request(url=url, data=data, is_aws=is_aws)\n",
    "    # create sse client\n",
    "    sse_client = sseclient.SSEClient(res)\n",
    "    # stream output\n",
    "    print(prompt, end = '')\n",
    "    for event in sse_client.events():\n",
    "        token = json.loads(event.data)[\"token\"][\"text\"]\n",
    "        print(token, end = split_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it and stream some token from SageMaker.\n",
    "\n",
    "### _NOTE: it seems that SageMaker is not yet supporting streaming/server send events. Compared to working HF sagemaker waits for the whole request to preprocessed and is not sending chunks back. Below is a second cell using the same code and an HF endpoint which works_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a adventures story about a John a middle age farmer:  something happened that disturbed him very much and he was depressed. The same day he was called to bring some land owned by the grandfather. But before he go, he found a hidden treasure by chance. But the landowner comes first to find the treasure and tried to shot him. John was left with a seriously injury in the brain.\n",
      "The landowner was taken to jail. The whole village welcome John as a hero. But this does not satisfy him.  In the night John was talking to himself ("
     ]
    }
   ],
   "source": [
    "# sageamker endpoint url\n",
    "url = f\"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{predictor.endpoint_name}/invocations\"\n",
    "\n",
    "prompt = \"Write a adventures story about a John a middle age farmer: \"\n",
    "request_payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 100,\n",
    "  }\n",
    "}\n",
    "\n",
    "# stream request\n",
    "# TODO streaming not working with SageMaker\n",
    "stream_request(url=url, data=request_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a adventures story about a John a middle age farmer: John is  a middle age farmer who sell s his dairy products in town . One day John decided to pick up  a few goat s that were being sold near his dairy . He called the owner of the goat s and inquire d about their prices . The owner of the goat s started to  y ell at John and refused to sell them to him . John was determined to protect his business , so  he grabbe d the goat s and started to ride them out of town . The goat s "
     ]
    }
   ],
   "source": [
    "# Example for flan hosted on HF with works\n",
    "url=\"https://flan-t5-xxl.ngrok.io/generate_stream\"\n",
    "\n",
    "# stream request\n",
    "stream_request(url=url, data=request_payload,is_aws=False,split_token=\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
